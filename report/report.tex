\documentclass[twocolumn]{../cs-classes/cs-classes}

\title{Extending Layer-wise Relevance Propagation\\ using Semiring Annotations}
\author{
    \textbf{Antoine Groudiev}\\L3, DI ENS, ENS\\\nolinkurl{antoine.groudiev@ens.psl.eu}
}
\date{}

\addbibresource{report.bib}
\graphicspath{{../cs-classes}{../images}}

\newcommand*{\K}{\mathbb{K}}
\newcommand*{\1}{\digitsbb{1}}
\newcommand*{\0}{\digitsbb{0}}

\usemintedstyle{colorful}

\begin{document}
\twocolumn[
  \begin{@twocolumnfalse}
    \maketitle
    \begin{abstract}
        Recently, neural networks allowed computers to solve numerous problems from diverse machine learning fields, such as natural language processsing and computer vision. Compared to traditional algorithms, machine learning models have proven both more successful and more difficult to interpret. Neural networks are considered as black boxes unable to easily explain themselves, that is justifying the reasons that led them to make a prediction. Layer-wise Relevance Propagation (LRP) is a technique that has been introduced to provide explanability by identifying the input features relevant to the output choice. In parallel, research in the databases field developed annotations techniques to compute provenance for queries. In this paper, we extend LRP propagation rules to semiring-based provenance annotations of the network, and implement semiring-based propagation rules for computer vision models of different scales.
    \end{abstract}
    \vspace*{1cm}
  \end{@twocolumnfalse}
]

\section{Introduction}
\subsection{Problem statement}
Deep neural networks have proven successful for solving with high accuracy machine learning problems. The expressivity of the class of functions generated by neural networks, combined with the relative simplicity of their training, make such models versatile tools to learn the relationship between the inputs and outputs of a dataset.

However, this versatility comes at the cost of poor interpretability: a neural network simply represents a function from one high-dimensional space to another, but provides no justification nor explanation for a given execution. If metrics such as the accuracy over a testing set provide confidence in the fact that the model is able to correctly classify inputs similar to the training set, no guarantee is given that the model generalizes well. Real-world examples show that networks can overfit the input data, or even take shortcuts instead of learning the intended solution \cite{shortcuts}. For the user to have confidence in its predictions, a neural network should therefore be able to highlight the patterns in the input data that it actually learned.

\subsection{State-of-the-Art review}
Explainable Artifical Intelligence techniques can be classified according to multiple criteras \cite{xai-survey}. Some methods consider the model as a black box and do not access the internal structure of the model \cite{fong2017interpretable}, while others take advantage of the internal states and weights of a network \cite{bach-2015,gradcam,circuits,inverting}. A distinction is also made between local and global explanations: a \emph{local} result explains a single execution of the model, while a \emph{global} result tries to understand the model for any possible execution \cite{visualizing-cnn}. Finally, \emph{self-explaining} approaches use only data made available by the model computation during prediction, while \emph{post-hoc} approaches perform more operations after the initial inference.

\subsection{Layer-wise Relevance Propagation}
Layer-wise Relevance Propagation (LRP) \cite{bach-2015} has been introduced as a technique to explain an execution of a neural network. LRP is a procedure propagating the output of the function backward in the network, using diverse rules to compute the \emph{relevance} of a neuron depending on the relevances of the neurons of the upstream layer. LRP introduces the notion ofÂ \emph{relevance score} for a neuron, intuitively quantifying the contribution of this neuron to the classification of final classification. A high relevance score indicates that the neuron led to the activation of the considered output; a negative relevance score represents neurons that increased the activation of another output neuron instead of the one considered.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{LRP.png}
    \caption{Illustration of LRP. The leftmost part corresponds to the forward pass, while relevance propagation is visualized in the rightmost part.\protect\footnotemark}
\end{figure}
\footnotetext{Figure adapted from \href{https://www.hhi.fraunhofer.de/en/departments/ai/technologies-and-solutions/layer-wise-relevance-propagation.html}{\say{Layer-wise Relevance Propagation}} -- Fraunhofer Institute for Telecommunications}

\subsubsection{Setup and notations}
In the following, we consider a deep neural network used for a classification task. We assume that it uses the rectifier activation function\footnote{That is $\ReLU(x)=\max(0, x)$, where ReLU stands for \emph{Rectified Linear Unit}.}, which is the case in most applications. To ease the notation, we will not consider biases but instead assume that the first neuron of each layer represents the bias. 

Let $L$ be the number of layers of the network, including the input and output layers. We denote by $\left(a^{(l)}_k\right)_k$ the activations of the network. Notably, $\left(a^{(1)}_k\right)_k$ is the input data, and $\left(a^{(L)}_k\right)_k$ is the ouput prediction. We denote by $\left(w^{(l)}_{j, k}\right)_{j, k}$ the weights connecting the $l$-th layer to the $(l+1)$-th layer. To simulate the biases using only weight matrices, we set:
\begin{equation*}
    \forall l\in\iset{1}{L}, \quad a^{(l)}_0 = 1
\end{equation*}
and we define $w^{(l)}_{0, k}$ to be the bias of the $k$-th neuron of the $(l+1)$-th layer. The forward propagation rule of a deep rectifier network is therefore:
\begin{equation}
    \forall l, \forall k, \quad a^{(l+1)}_k = \ReLU\left(\sum_{j}a^{(l)}_j w^{(l)}_{j, k}\right)
\end{equation}

We denote by $R^{(l)}_j$ the relevance of the $j$-th neuron of the $l$-th layer. We assume that the output layer represents a one-hot encoding, that is that the belonging of the input to the $i$-th class is represented by an output vector is of the form $(0, \dots, a^{(L)}_i, \dots, 0)$, where the only non-null coefficient is in the $i$-th position. Finally, we denote by $y$ the label of a classified input. To the label $y=i$ is associated the output vector $(0, \dots, a^{(L)}_i, \dots, 0)$.

\subsubsection{Propagation rules}
Relevance scores are initialized for the output layer, and are set to the output activation for the correct class, that is:
\begin{equation}
    R^{(L)}_i = \begin{cases*}
        a^{(L)}_i & if $i = y$\\
        0 & otherwise
    \end{cases*}
\end{equation}

The simplest LRP rule is called LRP-0. It propagates the relevance to a neuron of the lower layer proportionnaly to its contribution to each of the neuron of the next layer:
\begin{equation}
    \tag{LRP-0}
    R^{(l)}_j = \sum_{k}\frac{a^{(l)}_jw^{(l)}_{j, k}}{\sum_{j'}a^{(l)}_{j'}w^{(l)}_{j', k}} R^{(l+1)}_k
\end{equation}
The denominator $\sum_{j'}a^{(l)}_{j'}w^{(l)}_{j', k}$ guarantees a conservation property, that is that for any layer $l$:
\begin{equation*}
    \sum_j R^{(l)}_j = \sum_k R^{(l+1)}_k
\end{equation*}
This allows to keep the information regarding the total activation of the final layer.

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[scale=.6]
        \draw[thick] (0,0) -- (4, 2) node[above=.12, pos=.6] {$w_{j,1}$};
        \draw[thick] (0,0) -- (4, .7) node[below, pos=.6] {$w_{j,2}$};
        \draw[thick] (0,0) -- (4, -2) node[below=.12, pos=.6] {$w_{j,m}$};

        \node at (0, 1) {$a_j^{(l)}$};
        \draw[thick, fill=red!20,circle,minimum size=15] (0, 0) circle (10pt) node[left] {$R_j^{(l)}$};

        \draw[thick, fill=red!60,circle,minimum size=15] (4, 2) circle (10pt) node[right=.2] {$R_1^{(l+1)}$};
        \draw[thick, fill=red!10,circle,minimum size=15] (4, .7) circle (10pt) node[right=.2] {$R_2^{(l+1)}$};
        \node at (4, -.4) {$\vdots$};
        \draw[thick, fill=red!35,circle,minimum size=15] (4, -2) circle (10pt) node[right=.2] {$R_m^{(l+1)}$};
    \end{tikzpicture}
    \caption{Node-level illustration of relevance propagation.}
\end{figure}

The application of this simple rule can lead into noisy results that do not scale well. An overview of variations of LRP-0 is provided by \cite{montavon-lrp}; not all rules are suitable for all layers. Complex deep neural networks architectures benefit from enhanced rules such as LRP-$\epsilon$ or LRP-$\gamma$, which provide more stable explanations.

LRP-$\epsilon$ preserves only the most important parts of the explanation and reduces noise:
\begin{equation*}
    \tag{LRP-$\epsilon$}
    R^{(l)}_j = \sum_{k}\frac{a_jw_{j, k}}{\epsilon + \sum_{j'}a_{j'}w_{j', k}} R^{(l+1)}_k
\end{equation*}
where we ommited the dependency of $a$ and $w$ on the layer $l$. 

LRP-$\gamma$ favors positive contributions and stabilizes the explanation:
\begin{equation*}
    \tag{LRP-$\gamma$}
    R^{(l)}_j = \sum_{k}\frac{a_j\cdot(w_{j, k} + \gamma w_{j,k}^+)}{\sum_{j'}a_{j'}\cdot(w_{j', k}+ \gamma w_{j',k}^+)} R^{(l+1)}_k
\end{equation*}
where $(\cdot)^+=\max(0, \cdot)$.

Notably, the input layer must be handled using a different rule, since it does not receive its input from ReLU activations, but directly from the input data. The $z^\mathcal{B}$ rule is used in \cite{montavon-lrp} to propagate from layer 2 to 1 (input layer):
\begin{equation}
    R^{(1)}_j = \sum_{k} \frac{x_kw_{j, k} - l_jw^+_{j, k} - h_jw_{j, k}^-}{\sum_{j'}x_kw_{j', k} - l_jw^+_{j', k} - h_jw_{j', k}^-} R^{(2)}_k
\end{equation}
where $(\cdot)^+=\max(0, \cdot)$, $(\cdot)^-=\min(0, \cdot)$. The parameters $l_i$ and $h_i$ respectively define the theoretical minimum and maximum values of the inputs $x_i$. For instance, we might have $l_i=0$ and $h_i=255$ for pixels over 8 bits.

Best results are obtained using \emph{Composite LRP} \cite{montavon-lrp}, a method in which a specific rule is used for each layer: deepest layers use LRP-0, intermediate layers use LRP-$\epsilon$, and lower layers LRP-$\gamma$. The fact that some rules are empirically better for different layers can be explained 

\subsubsection{Classical LRP results}
Previous works (\cite{bach-2015}, \cite{montavon-lrp}), as well as the methods that we will introduce later on, were experimentaly tested on models trained over two datasets: fully-connected networks trained on the MNIST handwritten digits dataset \cite{mnist-dataset}, and deep convolutional networks pre-trained on the ImageNet visual database such as VGG-16 \cite{vgg}.

\autoref{fig:mnist-lrp} shows relevance for a simple model: a fully-connected deep rectifier network with layers of sizes $28\times28$, $300$, $100$ and $10$, trained on the MNIST dataset. Pixels highlighted in red have a positive relevance while blue pixels have a negative relevance. As expected, the relevant pixels to classify this image as a \texttt{0} are the white pixels in the input image.

\begin{figure}[H]
    \centering
    \includegraphics[width=.9\linewidth]{mnist-lrp.png}
    \caption{Input image and pixel-wise explanation of the output neuron \texttt{0}.}
    \label{fig:mnist-lrp}
\end{figure}

\autoref{fig:castle-lrp} provides a visualization for a more complex example, an execution of the VGG-16 network over a $224\times224$ image. 
\begin{figure}[H]
    \centering
    \begin{minipage}[c]{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{castle.jpg}
    \end{minipage}
    %\hspace*{-2.5cm}
    \begin{minipage}[c]{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{castle-lrp.png}
    \end{minipage}
    \caption{Input image\protect\footnotemark and relevance for the class "\texttt{castle}" of the VGG-16 network.}
    \label{fig:castle-lrp}
\end{figure}
\footnotetext{Image from the overview paper \cite{montavon-lrp}.}
Note that the castle part of the image is highlighted in red as intended. Furthermore, both the street sign and the street light have strong negative relevance; those two objects correspond to other classes of the ImageNet dataset (\texttt{street sign (919)} and \texttt{traffic light (920)}). Those two elements of the image would have positive relevance for LRP starting from the output neurons \texttt{919} and \texttt{920}.

\subsection{Semiring-based provenance annotations}
In parallel, the notion of data provenance in databases theory developed formal solutions to a similar problem to ours. Data provenance aims at \emph{explaning} a query by highlighting the tuples in the original database that led to the presence of a certain tuple in the query result. If contexts are different, deep neural networks explanation and data provenance share the same general setup: identifying a subset of the input that directly implied a certain output.

A framework to approach data provenance is \emph{provenance semirings}, introduced in \cite{green-2007}, annotates tuples using abstract elements of a semiring and apply semiring operations to the tuples appearing in the query. As the query is executed, information about the provenance of the intermediate results is aggregated, resulting in an abstract formula that can be concretized by substituting abstract elements and operations by a concrete semiring. Similarly, in the context of graph databases, edges can be annotated to derive a variety of properties of the query result. \cite{ramusat-prov}

In the following, we provide a mathematical definition of a semiring as well as semiring examples suited for the application to deep neural networks.

\begin{definition}[Semiring]
    A \emph{semiring} is an algebraic structure generalizing the notion of rings. A semiring $(\K, \oplus, \otimes, \0, \1)$ is composed of a set $\K$, binary operators $\oplus$ and $\otimes$ such that $\otimes$ distributes over $\oplus$, verifying the following properties:
    \begin{itemize}
        \item $(\K, \oplus, \0)$ is a commutative monoid
        \item $(\K, \otimes, \1)$ is a monoid such that $\0$ is absorbing
    \end{itemize}
\end{definition}

\begin{example}
    $(\R, +, \times, 0, 1)$ is a semiring. While it has no direct interpretation is the context of databases, we will see that is corresponds to the basic real-valued LRP.
\end{example}

\begin{example}[Boolean semiring]
    $(\B, \lor, \land, \bot, \top)$ where $\B:=\{\bot, \top\}$ is a semiring. Its use in databases provenance interprets as the existence of a path between two vertices, using edge weights as the number of different paths between two adjacent vertices. 
\end{example}

\begin{example}[Counting semiring]
    $(\mathbb{N}, +, \times, 0, 1)$ is a semiring. For a non-cyclic graph database, its use allows to compute the total number of paths between two vertices, using edge weights as the number of different paths between two adjacent vertices.
\end{example}

\begin{example}[Viertbi semiring]
    $([0, 1], \max, \times, 0, 1)$ is a semiring. For a non-cyclic graph database where the annotations are interpreted as a \say{confidence} measure, its use allows to compute the confidence score of the result of a query.
\end{example}

\section{Extending LRP using Semiring Annotations}
We aim at extending Layer-wise Relevance Propagation by annotating the computational graph of a neural network with semiring elements.

\subsection{Semiring generalization of the LRP rules}
LRP rules all inherit the same structure, in which the relevance of a pixel can be expressed as the weighted sum of the relevance of pixels of the next layer:
\begin{equation}
    R_j^{(l)} = \sum_k d_{j, k}^{(l)} \cdot R_k^{(l+1)}
    \label{eq:general-lrp}
\end{equation}
where the $d_{j,k}^{(l)}$ are some coefficients proportional to the extent to which neuron $j$ of the $l$-th layer has contribued to make neuron $k$ of the $(l+1)$-th layer relevant.

\autoref{eq:general-lrp} is the special real-valued case of a more general rule. Let $(\K, \oplus, \otimes, \0, \1)$ be a semiring, and $(\Theta^{(l)})_l$ be annotation functions, associating to each matrix of coefficients $d^{(l)}$ a matrix of semiring elements $\Theta^{(l)}(d)$. In particular, note that $d_{i,j} \mapsto \Theta^{(l)}(d)_{i,j}$ does not need to be a mapping: to identical coefficients can be associated different annotations. In the following examples, we will mostly use element-wise annotations functions of the form $\R\longrightarrow\K$.

We call $\K$-relevance for output neuron $y$ the quantity $R\in\K$ inductively defined by:
\begin{equation}
    R^{(L)}_i := \begin{cases*}
        \1 & if $i = y$\\
        \0 & otherwise
    \end{cases*}
    \label{eq:semiring-lrp-init}
\end{equation}
and
\begin{equation}
    \tag{$\K$-LRP}
    R^{(l)}_j := \bigoplus_{k}\Theta^{(l)}(d)_{j,k} \otimes R^{(l+1)}_k
    \label{eq:semiring-lrp-rec}
\end{equation}
Note that choosing $(\R,+,\times,0,1)$ as a semiring and $\Theta^{(l)}=x\mapsto x$ retrieves classical LRP up to a multiplicative factor of $a_i^{(L)}$ on each layer (which does not appear on the semiring initialization).

Equations \ref{eq:semiring-lrp-init} and \ref{eq:semiring-lrp-rec} can be intuitively interpreted in two ways. Firstly, they can be seen as the abstraction of an LRP computation using formal semiring elements. Computing $\K$-relevance for an execution of the network results in an abstract formula in terms of elements of $\K$ and operations $\oplus$ and $\otimes$. While this might be useful in the context of graph provenance, visualizing abstract formulas is difficult because of the size of computational graphs associated to neural networks. Furthermore, the general structure of the formulas is always the same for a specific model, since neural networks usually have very simple computational graphs structures. 

\begin{figure}[H]
    \centering
    \input{weights-graph.tex}
    \caption{A simple fully-connected neural network. The weights are represented on the edges.}
\end{figure}

Therefore, a second approach that fits the graph properties of neural networks is to interpret semiring LRP as operations on an annotated circuit, similarly to \cite{senellart2018provenance}. For instance, in the case of the boolean semiring, computing $\B$-relevance consists in taking for each node the logical conjunction of the annotations of its outgoing edges.

\begin{figure}[H]
    \centering
    \input{boolean-graph.tex}
    \caption{The same neural network where the edges have been annotated with elements of the boolean semiring.}
\end{figure}

Similarly to the implementation of provenance in databases by tools such as ProvSQL \cite{provsql}, we implemented a Python module on top of PyTorch\footnote{\href{https://pytorch.org}{\texttt{https://pytorch.org}}}, providing an interface to compute $\K$-relevance for any custom semiring $\K$. Details on the implementation are given in \autoref{sec:implementation}

\subsection{Results over the MNIST dataset}
We provide experimental results of the implementation of semiring-based LRP. In the following, we use the same fully-connected network as in \autoref{fig:mnist-lrp}, with layers of sizes $28\times28$, $300$, $100$ and $10$ and trained on the MNIST dataset.
\subsubsection{Boolean Semiring}
Boolean relevance can be computed using the boolean semiring $(\B=\{\bot, \top\}, \lor, \land, \bot, \top)$, and an element-wise annotation function $\Theta$ of the form:
\begin{equation*}
    \begin{aligned}
        \Theta: \R&\longrightarrow \B\\
        x&\longmapsto \begin{cases*}
            \top & if $x\geq\theta$\\
            \bot & otherwise
        \end{cases*}
    \end{aligned}
\end{equation*}
where $\theta$ is a hyperparameter called the threshold. A higher threshold reduces the number of $\top$ coefficients, ultimately reducing the number of input neurons having a relevance equal to $\top$. Choosing $\theta=10^{-9}$ filters out null and negative contributions to the output while maintaining a sufficient amount of activated inputs for visualization.

\begin{figure}[H]
    \centering
    \includegraphics[width=.9\linewidth]{boolean.png}
    \caption{Input image and $\B$-relevance for the output neuron \texttt{0} for $\theta=10^{-9}$.}
    \label{fig:mnist-boolean}
\end{figure}

The full propagation equation becomes:
\begin{equation}
    \tag{$\B$-LRP}
    R^{(l)}_j := \bigvee_{k}\Theta\left(\frac{a_j\cdot w_{j, k}}{\sum_{j'}a_{j'}\cdot w_{j', k}}\right) \land R^{(l+1)}_k
    \label{eq:boolean-lrp}
\end{equation}

$\B$-relevance provides a higher level explanation, highlighting large zones of the input image which contribute the most to the final classification.
Naturally, information about nuances in the contribution is lost. Intuitively, activated pixels (input neurons with relevance $\top$) are neurons such that there exists a \say{relevant} path from this neuron to the output neuron of the class \texttt{0}. A relevant path is a path in which all edges have a weight higher than the threshold $\theta$. While this condition might seem quite restrictive, note that computation graphs for neural networks are densely connected: there is $784\times300\times100=23\,520\,000$ different paths connecting one input pixel to the output neuron of the class \texttt{0}.

\subsubsection{Counting Semiring}
The counting semiring $(\mathbb{N}, +, \times, 0, 1)$ extends boolean relevance by enumerating the number of relevant paths starting from each input pixel. Its element-wise annotation function $\Theta$ is mostly identical:
\begin{equation*}
    \begin{aligned}
        \Theta: \R&\longrightarrow\N\\
        x&\longmapsto \begin{cases*}
            1 & if $x\geq\theta$\\
            0 & otherwise
        \end{cases*}
    \end{aligned}
\end{equation*}
but operations $+$ and $\times$ bring more expressivity to the framework, bringing more nuance in the highlighted zones. For small networks like the one studied, the number of relevant paths from input to output neurons appears to be roughly the same, making the final result qualitatively similar to \autoref{fig:mnist-boolean}.

\begin{figure}[H]
    \centering
    \includegraphics[width=.9\linewidth]{counting.png}
    \caption{Input image and $\N$-relevance for the output neuron \texttt{0} for $\theta=10^{-9}$. Highest relevance is 2098.}
\end{figure}

The full propagation equation becomes:
\begin{equation}
    \tag{$\N$-LRP}
    R^{(l)}_j := \sum_{k}\Theta\left(\frac{a_j\cdot w_{j, k}}{\sum_{j'}a_{j'}\cdot w_{j', k}}\right) \times R^{(l+1)}_k
    \label{eq:counting-lrp}
\end{equation}

\subsubsection{Viterbi Semiring}
The Viterbi semiring $([0, 1], \max, \times, 0, 1)$ replaces the $+$ of the classical real semiring by a $\max$. In a context where neurons can have the same weighted sum of relevance, taking the maximum of the relevance emphasizes the most important neurons, giving a more contrasted visualization than classical LRP. 
\begin{figure}[H]
    \centering
    \includegraphics[width=.9\linewidth]{viterbi.png}
    \caption{Input image and $[0, 1]$-relevance for the output neuron \texttt{0}.}
\end{figure}
To ensure that values belong to the $[0, 1]$ set, the following rule is used:
\begin{equation}
    R^{(l)}_j = \max_k \underbrace{\left(\frac{\left|a_j\cdot w_{j, k}\right|}{\max_{j'} \left|a_{j'}\cdot w_{j', k}\right|}\right)}_{\in[0, 1]} \cdot R^{(l+1)}_k
\end{equation}
In practice, a small amount such as \texttt{1e-9} can be added to the denominator to prevent from dividing by zero in the case where each $|a_{j'}\cdot w_{j',k}|$ is null.


\section{Applications}
We explore multiple applications of semiring-based LRP, going beyond the scope of simple execution-wise explanations. While LRP was intended as a tool to interpret a single execution of the network, we believe that averaging relevance scores of neurons over multiple executions can produce a meaningful metric of the \say{importance} of each neuron. Through three applications, we put forwards both qualitative and quantitative arguments supporting this view.

\subsection{Image mask computation}
The simplest setup consists in computing a \say{mask} by combining mulitple relevance results. For instance, \autoref{fig:boolean-mask} shows how the $\B$-relevances of multiple executions from the same class can be combined using boolean operators to build image masks. These masks delimitate spatial regions of the image that most likely contain meaningful features used to classify images as the class \texttt{0}. For instance, the most useful input pixels for the classification as \texttt{0} are spatially located in a ring around the center of the image.
\begin{figure}[H]
    \centering
    \includegraphics[width=.9\linewidth]{boolean-mask.png}
    \caption{Class-wise mask for $\B$-relevance of the class \texttt{0}.}
    \label{fig:boolean-mask}
\end{figure}
The same method also provides information on the internal neurons of the network that were used in the final classification. We believe that this technique can be used in the identification of \emph{circuits} inside the neural network \cite{circuits}, that is computational subgraphs of the network mostly responsible for one specific behavior of the model.

The spatial localisation of relevant pixels for a certain class do not scale to more complex models, where the dataset is not centered: if a certain object can be detected anywhere in the input image, the final mask will capture the entire image. Nevertheless, deeper neurons of networks are believed to be mostly spatially invariant, and mask computation can therefore be applied to the deepest layers.

To demonstrate the potential of mask computation using LRP, we built a toy example based on MNIST. We generated a new dataset in which images are made of two MNIST digits, one above the other (see \autoref{fig:dmnist-boolean-mask}). The label for each image is the uppermost digit; the lower digit has no influence over the label and is used simply to confuse the classifier. We expect the relevance of the lower part of the image to be null.
\begin{figure}[H]
    \centering
    \includegraphics[width=.85\linewidth]{dmnist-all-classes.png}
    \caption{Mask of the relevant part of the images, obtained by applying $\land$ and $\lor$ pixel-wise to the results of boolean LRP.}
    \label{fig:dmnist-all-classe}
\end{figure}
\autoref{fig:dmnist-all-classe} computes  a mask of the relevant part of the images by combining multiple $\B$-LRP results. Without prior knowledge of the connection between the label and the digits in the images, we can see that the model is learning to detect the upper digit in the image, and discards the lower digit.

The same procedure can be applied only for one class at a time, like in \autoref{fig:dmnist-boolean-mask}.
\begin{figure}[H]
    \centering
    \includegraphics[width=.9\linewidth]{dmnist-boolean-mask.png}
    \caption{Computation of the image mask, only for images classified as \texttt{0}.}
    \label{fig:dmnist-boolean-mask}
\end{figure}



% \begin{figure}[H]
%     \centering
%     \includegraphics[width=.5\linewidth]{boolean-mask-all.png}
%     \caption{All classes mask for $\B$-relevance}
% \end{figure}

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=.5\linewidth]{counting-mask.png}
%     \caption{Class-wise mask for $\N$-relevance of the class \texttt{0}}
% \end{figure}

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=.5\linewidth]{counting-mask-all.png}
%     \caption{All classes mask for $\N$-relevance}
% \end{figure}

\subsection{Network pruning using LRP ranking}
The accuracy of modern neural networks often come at the cost of large model size, increasing both storage, computational cost and inference time. To reduce the size of such models, an efficient technique called \emph{network pruning} has been studied \cite{pruning-survey,new-pruning}, in which neurons are removed from the network. A simple approach to selecting which neurons to prune is to select those with the smallest $\ell_1$ or $\ell_2$ norm: that is, those whose associated column in the weight matrix has the smallest norm. While this selection method yields good results, we show that removing neurons with the smallest average relevance -- in the sense of LRP -- allows the network to perform better.
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{pruning-graph-large.png}
    \caption{Accuracy and loss evolution during network pruning for different neuron selection methods.}
    \label{fig:pruning-graph}
\end{figure}

\autoref{fig:pruning-graph} compares multiple selection methods. For each method, the network was evaluated multiple times on a subset of the MNIST test set, each time with an increasing number of neurons pruned from the first layer, selected using the specific method. For LRP-based methods, the relevance of each neuron is computed by taking its average relevance on LRP results over images of the train set, and neurons with the lowest absolute relevance are pruned first.

Three curve clusters can be identified: boolean and counting LRP perform much worse than other selection methods, which can be explained by the lack of granularity in the explanation that these techniques provide. $\ell_1$ and $\ell_2$ norm selection methods provide a baseline to compare LRP techniques to, and both perform similarly well. Finally, classic LRP and Viterbi LRP outperform the norm-based methods by a small amount: this confirms the intuition that LRP provide a more subtle explanation than simply the magnitude of the weights connected to a neuron.

\subsection{Comparison to image perturbation}
In parallel to LRP, other methods aiming at providing a visual explanation of the decision of a classifier have emerged, such as \cite{inverting,visualizing-cnn}. An interesting approach, introduced in \cite{fong2017interpretable}, identifies pixels relevant to the classification by adding perturbations to the image. Intuitively, a pixel is relevant if the classification of the image changes when this pixel is modified. 

In \autoref{fig:mnist-attack}, we reproduced this idea by measuring the accuracy drop when different regions of the image are perturbed. Zones of the image of size $4\times4$ are modified by applying the transformation $x\longleftarrow255-x$ for pixels $x$ with values in the $\iset{0}{255}$ range. Darker zones show drops of accuracies when this zone of the image is perturbed.
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{attacks.png}
    \caption{Accuracy sensibility to perturbation by spatial region of the image. }
    \label{fig:mnist-attack}
\end{figure}
This results in a visualization of the critical zones of the image, qualitatively different from LRP. While LRP highlights the zones containing the classified object, perturbation methods mostly selects zones outside of the object, where change can lead to confusion with another class: for instance, the zone above the \texttt{4} is strongly highlighted since perburbation in this area might trick the model into believing that the digit is a \texttt{9} instead.

To evaluate to what extent LRP highlights pixels vulnerable to perturbations, we studied the impact of modifications in the zones of high relevance compared to low relevance zones. To do so, we measured the accuracy of the model while progressively perturbating more and more pixels of the image. In \autoref{fig:attack-graphs}, we compare the accuracy of the model when pixels are uniformly selected in the image, to the accuracy when pixels are selected with a probability distribution depending on the relevance. If $X$ is the random variable corresponding to the pixel selected to be modified, then:
\begin{equation}
    \P(X=i) = \frac{e^{-\eta\cdot R_i}}{\sum_je^{-\eta\cdot R_j}}
\end{equation}
In other terms, the probability distribution is the softmax of the opposite of the relevance; the higher the relevance, the lower the probability that the pixel is affected by a perburbation.
In this formula, $\eta$ is a \emph{relevance factor}, quantifying how much relevant pixels should be avoided compared to non-relevant pixels. This is especially important for semirings where the relevance values are close to 1, like the boolean or Viterbi semirings.
\begin{figure}[H]
    \includegraphics[width=.9\linewidth]{attack-graph-1.png}
    
    \includegraphics[width=.9\linewidth]{attack-graph-2.png}
    \caption{Accuracy drop for multiple pixels attacks strategies. Lower graph represents the difference to the uniform baseline.}
    \label{fig:attack-graphs}
\end{figure}
Results of \autoref{fig:attack-graphs} show that all LRP distributions outperform the uniform baseline. The Viterbi semiring performs worse than all the other LRP distributions, because of the important constrast between relevance scores. While highly relevant pixels are very unlikely to be attacked, some pixels with a low but non-zero relevance can still be attacked with a high probability.

To conclude, this experiment shows that while LRP does not directly highlight vulnerable pixels, pixels with less relevance are most likely to be resilient to perturbations.


\begin{figure*}
    \centering
    \input{vgg-architecture.tex}
    \caption{Architecture of the VGG-16 network.}
    \label{fig:vgg-architecture}
\end{figure*}
\section{Handling Convolutional Neural Networks}
If LRP rules are well-suited for simple, feedforward neural networks, most use cases of explanations occur with larger models, namely deep convolutional neural networks (CNNs). We explain how previously introduced rules can be adapted to CNNs, and provide visualization examples for the VGG-16 model.

\subsection{Computing relevance for convolutional layers}
% TODO: explain CNN forward pass somewhere?
Convolutional layers are special types of linear layers, in which an output neuron of a layer does not depend on every single neuron from the input. While in a rule of the form
\begin{equation*}
    R_j^{(l)} = \sum_k d_{j, k}^{(l)} \cdot R_k^{(l+1)}
\end{equation*}
the sum $\Sigma_k$ relates to every neuron of the output layer, the neuron $j$ might have no influence at all on most of the neurons in this sum. Therefore, we can restrict the sum to only neurons of the output layer which depend on the $j$-th neuron of the input layer. This can be seen in LRP rules, since the coefficients $d_{j, k}^{(l)}$ are proportional to $w_{j,k}$, which is null if there is no connection between neurons $j$ and $k$. 

If applying directly the formula to convolutional layers is theoretically possible, it is prohibitely expensive to do so in practice. Two methods allow to efficiently compute relevance for convolutional layers. We will first review an efficient gradient-based approach, that requires automatic differentiation; secondly, we will propose a slower method that can be used without depending on the gradient computation.

\subsubsection{Gradient expression of LRP}
Classical LRP rules can be easily and efficiently implemented. A general rule encapsulating LRP-0/$\epsilon$/$\gamma$ can be written as:
\begin{equation}
    R_j^{(l)} = \sum_k\frac{a_j\cdot\rho(w_{j,k})}{\epsilon+\sum_{j'}a_{j'}\cdot{\rho(w_{j',k})}} \cdot R_k^{(l+1)}
    \label{eq:general-lrp-full}
\end{equation}
where $a=a^{(l)}$, $w=w^{(l)}$. Four steps, described in \cite{montavon-lrp}, allow for an efficient and effortless computation of relevance:
\begin{equation*}
    \begin{aligned}
        \textstyle\forall k: z_k &= \epsilon + \sum\nolimits_{j'}a_{j'}\cdot\rho(w_{j',k})\\
        \forall k: s_k &= R_k / z_k\\
        \forall j: c_j &= \sum\nolimits_k\rho(w_{j',k})\cdot s_k\\
        \forall j: \!R_j &= a_j\cdot c_j
    \end{aligned}
\end{equation*}

These steps provide two majors benefits. Firstly, it allows for the costly divisions by $z_k$ to be performed only once per $k$, independently of $j$. Secondly, the third step can be expressed as the computation of a gradient:
\begin{equation}
    c_j = \left[\nabla\left(\sum\nolimits_k z_k(a^{(l)})\cdot s_k\right)\right]_j
    \label{eq:grad-lrp}
\end{equation}
where the gradient $\nabla$ is taken with respect to the vector $a^{(l)}$ and where the $s_k$ are treated as constants. This allows for a simple and efficient implementation, since the gradient can be computed using automatic differentiation: the high-level code is not only concise (two lines of PyTorch code), but also effortlessly generalizable  to any layer implementing backward pass, and highly optimized using GPU operations.

\subsubsection{Implementing LRP without automatic differentiation}
While the gradient-based expression is convenient for classical LRP, it does not allow for the overloading of sum and product operations, making it impossible to use with a semiring of our choice. To overcome this limitation, we explicitely derive the dependency between input and output neurons of the convolutional layer, resulting in less flexible code. 

Notice that the sum of contributions of one neuron to the output can be expressed as a convolution over the output. That is, in a formula of the form:
\begin{equation*}
    R_j^{(l)} = \sum_k \frac{z_{j,k}}{\sum_{j'}z_{j',k}} \cdot R_k^{(l+1)}
\end{equation*}
the sum in the denominator, $\sum_{j'}z_{j',k}$, can be expressed as a forward pass of the convolutional layer, that is a convolution over the input; in contrast, the main sum, $\sum_k$, can be expressed as a convolution over the output relevance.

% TODO: more on LRP convolution

\subsubsection{Other CNN layers}
Most deep convolutional neural networks contain other special layers. We provide a general description of how these were handled in our experiments on the VGG-16 model.

\paragraph*{Average Pooling}
Such layers can be seen as convolutional layers with fixed weights, in which each weight of the kernel is identical. In practice, most average pooling layers are used with a stride equal to the kernel size $K$, creating no overlapping. Therefore, the propagation rule for average pooling layers can be simplified as:
\begin{equation}
    R^{(l)}[i][j] := \Theta(d_{i,j}) \otimes R^{(l+1)}[i/K][j/K]
\end{equation}
where $K$ is the kernel size.

\paragraph*{Batch Normalization Layers}
At test time, normalization layers act as a linear operatioin, and can therefore be fused with the previous linear layer. LRP and semiring-based LRP can then be applied directly to the fused layer.

\begin{figure*}
    \centering
    \includegraphics[width=.8\linewidth]{vgg-boolean.png}
    \caption{$\B$-relevance of the class \texttt{castle}.}
    \label{fig:vgg-boolean}
\end{figure*}
\subsection{Results for the VGG-16 network}
We conducted experiments on the VGG-16 network \cite{vgg}, trained on the ImageNet-1K dataset, containing 1\,281\,167 images from 1\,000 classes. Its very deep architecture (see \autoref{fig:vgg-architecture}) makes it a challenging model for relevance propagation, since minor changes in the deeper layers can ruin the visualization of the first layers.

A necessary ajustement compared to the simpler fully-connected neural networks is to be able to choose a different annotation function for each layer. Instead of using the same $\Theta$ at every layer, a family of functions $(\Theta^{(l)})_l$ is used. In practice, since we used threshold-based annotation functions, this means that each layer will have a different threshold. We discuss the choice of thresholds and the correlation to other parameters of the network in \autoref{sec:thresholds-choice}.

\subsubsection{Boolean semiring}
$\B$-relevance over VGG-16 (\autoref{fig:vgg-boolean}) is coherent with results on the MNIST dataset: it specifies a main zone of importance in the image, similarly to explainations of tools such as Grad-CAM \cite{gradcam}. The input layer having three channels (R, G, B), they are aggregated using OR/AND operations.

While classical LRP provides a more precise explanation by focusing on the contours of the relevant object, $\B$-relevance selects all the pixels of the object, including its center. From a human point of view, there is no reason to believe that the contours of the object are more relevant than the center of the object. Therefore, boolean LRP allows for a larger explanation that is closer to what a human would produce.

\subsubsection{Counting semiring}
Counting semiring (\autoref{fig:vgg-counting}) produces an explanation qualitatively similar to classical LRP. Compared to Composite LRP, the choice of thresholds limits the propagation of relevance to provide a more localised explanation. This avoids the inclusion of less relevant elements such as the street light on top of the image.
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{vgg-counting.png}
    \caption{$\N$-relevance of the class \texttt{castle}.}
    \label{fig:vgg-counting}
\end{figure}

\subsubsection{Viterbi semiring}
The Viterbi semiring relevance appears to scale poorly to deep convolutional neural networks. Comparatively to counting or boolean semirings, it does not use thresholds to limit the spread of relevance during convolutions, which leads to the uniformization of relevance throughout the image. Therefore, the explanation obtained on the input layer is not humanly interpretable, as it concentrated on a very small number of pixels -- pixels that aren't even relevant to the classification.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{vgg-viterbi-0.png}
    \caption{Viterbi relevance of the class \texttt{castle} for the layer 0.}
\end{figure}

Nevertheless, an interesting aspect of CNNs is that the intermediate LRP results are also visualizable, since they respect spatial information. We can therefore display the relevance for a deep layer (in \autoref{fig:vgg-viterbi-28}, the 28th layer) to understand why the relevance propagation failed with this semiring.\footnote{Note that it remains harder to interpret deeper layers because of the way that channels are aggregated: here, we always fuzed channels by summing them, but this approach works better for 3 channels (input layer) than for 512 channels (28th layer).}
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{vgg-viterbi-28.png}
    \caption{Viterbi relevance of the class \texttt{castle} for the layer 28.}
    \label{fig:vgg-viterbi-28}
\end{figure}
In this case, we can see that even at the begining of the propagation, the relevance is already spread out in the image. Information about the specific zone of the castle is already lost, and cannot be recovered by the lowest layers.

\subsection{Thresholds choice}
\label{sec:thresholds-choice}
Equations such as \ref{eq:boolean-lrp} and \ref{eq:counting-lrp} introduce thresholds which must be ajusted to provide meaningful explanation. Selecting such parameters optimally requires a proper definition of explanation quality, which is a separate research domain \cite{samek2016evaluating}. Nevertheless, only some ranges of threshold values guarantee a final result that is humanly interpretable. Threshold values that are too high result in visualizations containing little to no information; relevance is \say{lost} in the deepest layers and only a few input neurons are marked as relevant. Threshold values that are too low result in over-saturated visualizations; for instance, boolean relevance selects most neurons as relevant, loosing meaningful information in the amount of activated pixels.

We propose threshold values, used in \autoref{fig:vgg-boolean}, that offer a balanced explanation. Modifiying these values by small amounts allow the user to choose between a more precise or more general explanation.
While those parameters were chosen manually, they appear to be correlated to parameters of the convolutional layers. 
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{threshold-kernel-comparison.png}
    \caption{Manually-chosen thresholds of convolutional layers used to compute $\B$-relevance, and kernel size of the convolutional layer.}
\end{figure}
The general variation of thresholds follows the one of the kernel size of the layer: very low between the 37th and 31st layers, peaking at the 28th, and maintaining medium values between the 24th and first layers. The correlation between those two characteristics suggests a general rule to pick the thresholds, the detailled values of which are left to the user to pick its prefered explanation.

\subsection{Notes on implementation}
\label{sec:implementation}
Finally, we provide some details on the implementation work associated with the previous results. All previous experimental results are implemented in Python, notably using the \href{https://pytorch.org}{\texttt{PyTorch}} and \href{https://numpy.org}{\texttt{NumPy}} modules. The integrality of the source code can be found at \href{https://github.com/Red-Rapious/Semiring-LRP}{\texttt{https://github.com/Red-Rapious/Semiring-LRP}}; LRP implementation is inspired by the code\footnote{\href{https://heatmapping.org/tutorial}{\texttt{https://heatmapping.org/tutorial}}} from the LRP review paper \cite{montavon-lrp}.

A tutorial, in the form of a Jupyter Notebook, is available \href{https://github.com/Red-Rapious/Semiring-LRP/tutorial/tutorial.ipynb}{on GitHub}\footnote{\href{https://github.com/Red-Rapious/Semiring-LRP/tutorial/tutorial.ipynb}{\nolinkurl{https://github.com/Red-Rapious/Semiring-LRP/tutorial/tutorial.ipynb}}}. It details a \texttt{NumPy}-based implementation of Semiring LRP for an MNIST MLP and VGG-16.

A straighforward approach to implementing Semiring LRP into existing models is to define a custom \texttt{Semiring} class providing the different customizable elements:
\begin{minted}[mathescape=true]{python}
class Semiring:
    # for annotation
    def __init__(x): # $\Theta$
        return ...

    def __add__(self, b): # $\oplus$
        return ...

    def __mul__(self, b): # $\otimes$
        return ...

    def zero(): # $\0$
        return ...

    def one(): # $\1$
        return ...
\end{minted}
Note that the methods \mintinline{python}{__add__} and \mintinline{python}{__mul__} overload the operations $+$ and $\times$, and allow to directly write expressions such as \mintinline{python}{a + b} where \mintinline{python}{a} and \mintinline{python}{b} are elements of the class \mintinline{ocaml}{Semiring}.

LRP initialization can be written using the \mintinline{python}{zero} and \mintinline{python}{one} class methods. One propagation step of the form of \autoref{eq:general-lrp-full} can be computed using:
\begin{minted}[mathescape=true]{python}
# Apply $\rho$ to the weigts and biases
w = rho(W[l], l)
b = rho(B[l], l)

# Compute the denominator
# incr adds $\epsilon$ to the sum
z = incr(A[l].dot(w)+b, l)

# Compute each coefficient $d_{j,k}$
d = (A[l] * w.T).T/z[] 

# Replace floats by their annotations
d = Semiring.annotate(d)

# Compute the sum $\sum_k$
R[l] = d @ R[l+1]
\end{minted}

A similar implementation for convolutional layers can be implemented by coding the convolution process by hand.

Note that relying on high-level Python code yields poor performance, especially compared to the gradient-based implementation described in \autoref{eq:grad-lrp}. Such a problem could be avoided with a lower-level implementation in the core of \texttt{PyTorch}, similarly to the implementation of ProvSQL \cite{provsql}.

\section{Conclusion}
We introduced Semiring-based Layer-wise Relevance Propagation to extend LRP to a broader range of explanations. Semiring-based LRP benefits from the capabilities of classical LRP, while letting the user choose customized semirings for explanations that better suit the applications of its model. Experimentally, boolean and counting semirings provide scalable explanations that qualitatively differ from classical LRP.

The current work shows that the semiring generalization of LRP encapsulates differnt explanations types that go beyond classical LRP. To prove useful to larger-scale applications, it might be interesting for the framework to support diverse new types of layers; we proved that Semiring-LRP could scale to CNNs, but architectures such as RNNs or transformers could benefit from this framework.

Finally, scalability raises the challenge of performance, as the current high-level implementation would be too slow for most applications. As discussed previously, lower-level \texttt{PyTorch} would most likely result in reasonable performance.

%We introduced Semiring-based Layer-wise Relevance Propagation to extend LRP to a broader range of explanations. Semiring-based LRP benefits from the capabilities of classical LRP, while letting the user choose customized semirings for explanations that better suit the applications of its model. 

%We implemented Semiring-based LRP for PyTorch models containing Linear, Convolutional, Spatial pooling, and Activation layers; the user can simply implement its own semiring by overloading the addition, multiplication, $\0$ and $\1$ elements and annotation function. Experimentally, boolean and counting semiring provide scalable explanations that qualitatively differ from classical LRP.

%Finally, we explored three examples of applications of semiring-based LRP: mask computation, network pruning, and vulnerable pixels detection. We believe that semiring-based LRP has the potential to provide more than an explanation for a single execution, and those applications tend to show that combining LRP results over multiple executions produces meaningful metrics for the overall relevance of neurons in the network. 

\addcontentsline{toc}{section}{Acknowldgements}
\section*{Acknowldegements}
I would like to thank Silviu Maniu, my advisor, for his guidance and support throughout this internship. I thank Pierre Senellart for his feedback and advices. Thanks to the LIG and specifically to the members of the SLIDE team. In particular, thanks to Alan Gany, Nassim Bouarour for their help, and thanks to Paul Landrier for his suggestions.

\newpage
\addcontentsline{toc}{section}{References}
\nocite{*}
\printbibliography

\newpage
% \vfill\null
% \newpage

\begingroup
\hypersetup{hidelinks}
\tableofcontents
\newpage
\listoffigures
\endgroup

\appendix

\end{document}